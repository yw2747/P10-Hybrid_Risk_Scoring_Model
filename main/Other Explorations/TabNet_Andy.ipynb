{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "with open(\"train_val_test_1.pkl\", \"rb\") as f:\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = pickle.load(f)\n",
    "\n",
    "print(\"Data successfully loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 12190772 missing values out of 54892538 elements (22.21%)\n",
      "Validation set: 150872 missing values out of 21956968 elements (0.69%)\n",
      "Test set: 163676 missing values out of 21956968 elements (0.75%)\n",
      "\n",
      "Top columns with missing values:\n",
      "- il_util: 857435 missing values (92.16%)\n",
      "- mths_since_rcnt_il: 848672 missing values (91.22%)\n",
      "- all_util: 846446 missing values (90.98%)\n",
      "- inq_last_12m: 846444 missing values (90.98%)\n",
      "- total_cu_tl: 846444 missing values (90.98%)\n",
      "- open_acc_6m: 846444 missing values (90.98%)\n",
      "- open_act_il: 846443 missing values (90.98%)\n",
      "- max_bal_bc: 846443 missing values (90.98%)\n",
      "- inq_fi: 846443 missing values (90.98%)\n",
      "- open_il_12m: 846443 missing values (90.98%)\n",
      "\n",
      "First 5 rows of data:\n",
      "       acc_open_past_24mths  all_util  annual_inc  application_type  \\\n",
      "39785                   NaN       NaN     22000.0                 0   \n",
      "39763                   NaN       NaN    150000.0                 0   \n",
      "39751                   NaN       NaN    125000.0                 0   \n",
      "39752                   NaN       NaN     40000.0                 0   \n",
      "39758                   NaN       NaN     20000.0                 0   \n",
      "\n",
      "       avg_cur_bal  bc_open_to_buy  bc_util    dti  delinq_2yrs  emp_length  \\\n",
      "39785          NaN             NaN      NaN  14.29          1.0         0.0   \n",
      "39763          NaN             NaN      NaN   0.00          0.0         8.0   \n",
      "39751          NaN             NaN      NaN   0.27          0.0        10.0   \n",
      "39752          NaN             NaN      NaN   2.55          0.0         6.0   \n",
      "39758          NaN             NaN      NaN   2.04          0.0         0.0   \n",
      "\n",
      "       ...  total_bc_limit  tot_hi_cred_lim  total_bal_il  purpose  \\\n",
      "39785  ...             NaN              NaN           NaN        2   \n",
      "39763  ...             NaN              NaN           NaN        4   \n",
      "39751  ...             NaN              NaN           NaN        2   \n",
      "39752  ...             NaN              NaN           NaN        0   \n",
      "39758  ...             NaN              NaN           NaN        1   \n",
      "\n",
      "       total_rev_hi_lim  earliest_cr_line  total_cu_tl  open_rv_12m  pub_rec  \\\n",
      "39785               NaN               257          NaN          NaN      0.0   \n",
      "39763               NaN               447          NaN          NaN      0.0   \n",
      "39751               NaN               458          NaN          NaN      0.0   \n",
      "39752               NaN               481          NaN          NaN      0.0   \n",
      "39758               NaN               250          NaN          NaN      0.0   \n",
      "\n",
      "       open_il_12m  \n",
      "39785          NaN  \n",
      "39763          NaN  \n",
      "39751          NaN  \n",
      "39752          NaN  \n",
      "39758          NaN  \n",
      "\n",
      "[5 rows x 59 columns]\n"
     ]
    }
   ],
   "source": [
    "def check_missing_values(X_train, X_val, X_test):\n",
    "    train_elements = X_train.size if isinstance(X_train, np.ndarray) else X_train.size\n",
    "    val_elements = X_val.size if isinstance(X_val, np.ndarray) else X_val.size\n",
    "    test_elements = X_test.size if isinstance(X_test, np.ndarray) else X_test.size\n",
    "    \n",
    "    train_missing = np.isnan(X_train).sum() if isinstance(X_train, np.ndarray) else X_train.isna().sum().sum()\n",
    "    val_missing = np.isnan(X_val).sum() if isinstance(X_val, np.ndarray) else X_val.isna().sum().sum()\n",
    "    test_missing = np.isnan(X_test).sum() if isinstance(X_test, np.ndarray) else X_test.isna().sum().sum()\n",
    "    \n",
    "    train_pct = 100 * train_missing / train_elements\n",
    "    val_pct = 100 * val_missing / val_elements\n",
    "    test_pct = 100 * test_missing / test_elements\n",
    "    \n",
    "    print(f\"Training set: {train_missing} missing values out of {train_elements} elements ({train_pct:.2f}%)\")\n",
    "    print(f\"Validation set: {val_missing} missing values out of {val_elements} elements ({val_pct:.2f}%)\")\n",
    "    print(f\"Test set: {test_missing} missing values out of {test_elements} elements ({test_pct:.2f}%)\")\n",
    "    \n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        missing_cols = X_train.isna().sum().sort_values(ascending=False)\n",
    "        missing_cols = missing_cols[missing_cols > 0]\n",
    "        \n",
    "        print(\"\\nTop columns with missing values:\")\n",
    "        if len(missing_cols) > 0:\n",
    "            for col, count in missing_cols[:10].items():\n",
    "                pct = 100 * count / len(X_train)\n",
    "                print(f\"- {col}: {count} missing values ({pct:.2f}%)\")\n",
    "        else:\n",
    "            print(\"No columns with missing values!\")\n",
    "    \n",
    "    return train_missing, val_missing, test_missing\n",
    "\n",
    "train_missing, val_missing, test_missing = check_missing_values(X_train, X_val, X_test)\n",
    "\n",
    "if isinstance(X_train, pd.DataFrame):\n",
    "    print(\"\\nFirst 5 rows of data:\")\n",
    "    print(X_train.head())\n",
    "else:\n",
    "    print(\"\\nShape of training data:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After splitting but before modeling:\n",
    "\n",
    "# 1. Check missing value percentage in each split\n",
    "train_missing = (X_train.isna().sum() / len(X_train)) * 100\n",
    "val_missing = (X_val.isna().sum() / len(X_val)) * 100\n",
    "test_missing = (X_test.isna().sum() / len(X_test)) * 100\n",
    "\n",
    "# 2. For columns with high missing rates, create indicator features\n",
    "high_missing_cols = train_missing[train_missing > 30].index\n",
    "for col in high_missing_cols:\n",
    "    X_train[f'{col}_missing'] = X_train[col].isna().astype(int)\n",
    "    X_val[f'{col}_missing'] = X_val[col].isna().astype(int)\n",
    "    X_test[f'{col}_missing'] = X_test[col].isna().astype(int)\n",
    "\n",
    "# 3. Apply imputation consistently\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')  # or mean, mode, etc.\n",
    "imputer.fit(X_train)  # Fit only on training data\n",
    "\n",
    "X_train = imputer.transform(X_train)\n",
    "X_val = imputer.transform(X_val)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: [169418 760964]\n",
      "Class weight ratio: 0.22\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Calculate class imbalance \n",
    "class_counts = np.bincount(y_train)\n",
    "print(f\"Class distribution: {class_counts}\")\n",
    "class_weight = class_counts[0] / class_counts[1]\n",
    "print(f\"Class weight ratio: {class_weight:.2f}\")\n",
    "\n",
    "# If pd df convert to np arr\n",
    "if isinstance(X_train, pd.DataFrame):\n",
    "    X_train_values = X_train.values\n",
    "    X_val_values = X_val.values\n",
    "    X_test_values = X_test.values\n",
    "else:\n",
    "    X_train_values = X_train\n",
    "    X_val_values = X_val\n",
    "    X_test_values = X_test\n",
    "\n",
    "if isinstance(y_train, pd.Series):\n",
    "    y_train_values = y_train.values\n",
    "    y_val_values = y_val.values\n",
    "    y_test_values = y_test.values\n",
    "else:\n",
    "    y_train_values = y_train\n",
    "    y_val_values = y_val\n",
    "    y_test_values = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tabnet_model = TabNetClassifier(\\n    n_d=32,  # Reduced from 64\\n    n_a=32,  # Reduced from 64\\n    n_steps=3,  # Reduced from 5\\n    gamma=1.5,  # Scaling factor for attention\\n    n_independent=1,  # Reduced from 2\\n    n_shared=1,  # Reduced from 2\\n    momentum=0.3,\\n    mask_type=\\'entmax\\',\\n    lambda_sparse=1e-3,\\n    optimizer_fn=torch.optim.Adam,\\n    optimizer_params=dict(lr=2e-2),\\n    scheduler_params=dict(\\n        mode=\"min\",\\n        patience=10,\\n        min_lr=1e-5,\\n        factor=0.5\\n    ),\\n    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\\n    verbose=1,\\n    device_name=\\'mps\\'  \\n)\\n\\n# Weights to handle class imbalance\\nweights = np.ones(y_train_values.shape[0])\\nweights[y_train_values == 1] = class_weight\\n\\nstart_time = time.time()\\n\\ntabnet_model.fit(\\n    X_train_values, y_train_values,\\n    eval_set=[(X_val_values, y_val_values)],\\n    max_epochs=50,  # Reduced from 200\\n    patience=10,  # Reduced from 30\\n    batch_size=2048,  # Increased from 1024\\n    weights=weights,  # Apply class weights\\n    eval_metric=[\"auc\", \"accuracy\"]\\n)\\n\\ntraining_time = time.time() - start_time\\nprint(f\"TabNet training completed in {training_time:.2f} seconds.\")'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"tabnet_model = TabNetClassifier(\n",
    "    n_d=32,  # Reduced from 64\n",
    "    n_a=32,  # Reduced from 64\n",
    "    n_steps=3,  # Reduced from 5\n",
    "    gamma=1.5,  # Scaling factor for attention\n",
    "    n_independent=1,  # Reduced from 2\n",
    "    n_shared=1,  # Reduced from 2\n",
    "    momentum=0.3,\n",
    "    mask_type='entmax',\n",
    "    lambda_sparse=1e-3,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params=dict(\n",
    "        mode=\"min\",\n",
    "        patience=10,\n",
    "        min_lr=1e-5,\n",
    "        factor=0.5\n",
    "    ),\n",
    "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    verbose=1,\n",
    "    device_name='mps'  \n",
    ")\n",
    "\n",
    "# Weights to handle class imbalance\n",
    "weights = np.ones(y_train_values.shape[0])\n",
    "weights[y_train_values == 1] = class_weight\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "tabnet_model.fit(\n",
    "    X_train_values, y_train_values,\n",
    "    eval_set=[(X_val_values, y_val_values)],\n",
    "    max_epochs=50,  # Reduced from 200\n",
    "    patience=10,  # Reduced from 30\n",
    "    batch_size=2048,  # Increased from 1024\n",
    "    weights=weights,  # Apply class weights\n",
    "    eval_metric=[\"auc\", \"accuracy\"]\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"TabNet training completed in {training_time:.2f} seconds.\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 weight: 8.983272143455832, Class 1 weight: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights properly\n",
    "n_samples = len(y_train_values)\n",
    "n_class_1 = sum(y_train_values == 1)\n",
    "n_class_0 = n_samples - n_class_1\n",
    "\n",
    "# If class 0 is the minority class\n",
    "if n_class_0 < n_class_1:\n",
    "    # Weight class 0 (minority) higher - a more moderate approach\n",
    "    class_0_weight = n_class_1 / n_class_0 * 2  # More moderate multiplier\n",
    "    class_1_weight = 1.0\n",
    "else:\n",
    "    # If class 1 is the minority class\n",
    "    class_0_weight = 1.0\n",
    "    class_1_weight = n_class_0 / n_class_1 * 2\n",
    "\n",
    "# Apply weights\n",
    "weights = np.ones(y_train_values.shape[0])\n",
    "weights[y_train_values == 0] = class_0_weight\n",
    "weights[y_train_values == 1] = class_1_weight\n",
    "\n",
    "print(f\"Class 0 weight: {class_0_weight}, Class 1 weight: {class_1_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andy/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.65411 | val_0_auc: 0.63964 | val_0_accuracy: 0.53584 |  0:00:43s\n",
      "epoch 1  | loss: 0.63432 | val_0_auc: 0.67464 | val_0_accuracy: 0.56617 |  0:01:26s\n",
      "epoch 2  | loss: 0.62738 | val_0_auc: 0.68965 | val_0_accuracy: 0.59908 |  0:02:09s\n",
      "epoch 3  | loss: 0.62302 | val_0_auc: 0.69849 | val_0_accuracy: 0.59104 |  0:02:52s\n",
      "epoch 4  | loss: 0.62117 | val_0_auc: 0.70745 | val_0_accuracy: 0.63459 |  0:03:36s\n",
      "epoch 5  | loss: 0.61857 | val_0_auc: 0.71139 | val_0_accuracy: 0.60272 |  0:04:19s\n",
      "epoch 6  | loss: 0.61734 | val_0_auc: 0.71299 | val_0_accuracy: 0.64882 |  0:05:01s\n",
      "epoch 7  | loss: 0.6171  | val_0_auc: 0.71419 | val_0_accuracy: 0.66815 |  0:05:44s\n",
      "epoch 8  | loss: 0.61689 | val_0_auc: 0.71259 | val_0_accuracy: 0.66797 |  0:06:27s\n",
      "epoch 9  | loss: 0.61508 | val_0_auc: 0.71477 | val_0_accuracy: 0.63915 |  0:07:10s\n",
      "epoch 10 | loss: 0.61471 | val_0_auc: 0.71384 | val_0_accuracy: 0.62091 |  0:07:54s\n",
      "epoch 11 | loss: 0.61351 | val_0_auc: 0.71472 | val_0_accuracy: 0.62327 |  0:08:38s\n",
      "epoch 12 | loss: 0.61392 | val_0_auc: 0.71537 | val_0_accuracy: 0.62553 |  0:09:22s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 7 and best_val_0_accuracy = 0.66815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andy/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet training completed in 626.74 seconds.\n"
     ]
    }
   ],
   "source": [
    "tabnet_model = TabNetClassifier(\n",
    "    n_d=24,  # Slightly increased from 8\n",
    "    n_a=24,  # Slightly increased from 8\n",
    "    n_steps=3,  # Increased from 1\n",
    "    gamma=1.5,\n",
    "    n_independent=1,\n",
    "    n_shared=1,\n",
    "    momentum=0.3,\n",
    "    mask_type='entmax',\n",
    "    lambda_sparse=1e-3,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,  # Added back\n",
    "    scheduler_params=dict(\n",
    "        mode=\"min\",\n",
    "        patience=3,\n",
    "        min_lr=1e-5,\n",
    "        factor=0.5\n",
    "    ),\n",
    "    verbose=1,  # Set to 1 to see progress\n",
    "    device_name='cpu'\n",
    ")\n",
    "\n",
    "# Use class weighting again\n",
    "weights = np.ones(y_train_values.shape[0])\n",
    "weights[y_train_values == 1] = class_weight\n",
    "\n",
    "# Use more data - 50% instead of 20%\n",
    "sample_size = int(X_train_values.shape[0] * 1)\n",
    "indices = np.random.choice(X_train_values.shape[0], sample_size, replace=False)\n",
    "X_sample = X_train_values[indices]\n",
    "y_sample = y_train_values[indices]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "tabnet_model.fit(\n",
    "    X_sample, y_sample,\n",
    "    eval_set=[(X_val_values, y_val_values)],  # Use full validation set\n",
    "    max_epochs=20,  # More epochs\n",
    "    patience=5,  # More patience\n",
    "    batch_size=2048,  # Slightly smaller batch\n",
    "    weights=weights[indices],  # Use weights\n",
    "    eval_metric=[\"auc\", \"accuracy\"]  # Track both metrics\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"TabNet training completed in {training_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TabNet feature importances:\n",
      "Feature 70: 0.2332\n",
      "Feature 62: 0.1833\n",
      "Feature 0: 0.1493\n",
      "Feature 50: 0.1213\n",
      "Feature 31: 0.0595\n",
      "Feature 58: 0.0419\n",
      "Feature 3: 0.0414\n",
      "Feature 60: 0.0281\n",
      "Feature 6: 0.0266\n",
      "Feature 21: 0.0158\n"
     ]
    }
   ],
   "source": [
    "feature_importances = tabnet_model.feature_importances_\n",
    "if isinstance(X_train, pd.DataFrame):\n",
    "    feature_names = X_train.columns.tolist()\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    print(\"\\nTop 10 most important features (TabNet):\")\n",
    "    print(importance_df.head(10))\n",
    "else:\n",
    "    print(\"\\nTabNet feature importances:\")\n",
    "    top_indices = np.argsort(feature_importances)[-10:][::-1]\n",
    "    for idx in top_indices:\n",
    "        print(f\"Feature {idx}: {feature_importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.6599\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 51009  31394]\n",
      " [ 95169 194580]]\n",
      "Class 0 accuracy: 0.6190\n",
      "Class 1 accuracy: 0.6715\n",
      "Successfully saved model at tabnet_model.zip.zip\n",
      "\n",
      "Model saved to tabnet_model.zip\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = tabnet_model.predict_proba(X_test_values)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_values, y_pred)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test_values, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "class_0_acc = cm[0,0] / (cm[0,0] + cm[0,1]) if (cm[0,0] + cm[0,1]) > 0 else 0\n",
    "class_1_acc = cm[1,1] / (cm[1,0] + cm[1,1]) if (cm[1,0] + cm[1,1]) > 0 else 0\n",
    "print(f\"Class 0 accuracy: {class_0_acc:.4f}\")\n",
    "print(f\"Class 1 accuracy: {class_1_acc:.4f}\")\n",
    "\n",
    "model_path = \"tabnet_model.zip\"\n",
    "tabnet_model.save_model(model_path)\n",
    "print(f\"\\nModel saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = TabNetClassifier()\n",
    "# loaded_model.load_model(\"tabnet_model.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
