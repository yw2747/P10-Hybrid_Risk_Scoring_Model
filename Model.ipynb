{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "with open(\"train_val_test.pkl\", \"rb\") as f:\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = pickle.load(f)\n",
    "\n",
    "print(\"Data successfully loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical features to category dtype for XGBoost\n",
    "categorical_features = [\"sub_grade\", \"term\", \"purpose\"]\n",
    "for col in categorical_features:\n",
    "    X_train[col] = X_train[col].astype(\"category\")\n",
    "    X_val[col] = X_val[col].astype(\"category\")\n",
    "    X_test[col] = X_test[col].astype(\"category\")\n",
    "\n",
    "# One-Hot Encode \"emp_length\" since it has many categories\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "encoded_train = ohe.fit_transform(X_train[[\"emp_length\"]])\n",
    "encoded_val = ohe.transform(X_val[[\"emp_length\"]])\n",
    "encoded_test = ohe.transform(X_test[[\"emp_length\"]])\n",
    "\n",
    "# Convert encoded data to DataFrame\n",
    "encoded_train_df = pd.DataFrame(encoded_train, index=X_train.index, columns=ohe.get_feature_names_out([\"emp_length\"]))\n",
    "encoded_val_df = pd.DataFrame(encoded_val, index=X_val.index, columns=ohe.get_feature_names_out([\"emp_length\"]))\n",
    "encoded_test_df = pd.DataFrame(encoded_test, index=X_test.index, columns=ohe.get_feature_names_out([\"emp_length\"]))\n",
    "\n",
    "# Drop original \"emp_length\" and concatenate encoded features\n",
    "X_train = X_train.drop(columns=[\"emp_length\"]).reset_index(drop=True)\n",
    "X_val = X_val.drop(columns=[\"emp_length\"]).reset_index(drop=True)\n",
    "X_test = X_test.drop(columns=[\"emp_length\"]).reset_index(drop=True)\n",
    "\n",
    "X_train = pd.concat([X_train, encoded_train_df], axis=1)\n",
    "X_val = pd.concat([X_val, encoded_val_df], axis=1)\n",
    "X_test = pd.concat([X_test, encoded_test_df], axis=1)\n",
    "\n",
    "print(\"Categorical processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective=\"multi:softmax\",  # Multi-class classification\n",
    "    num_class=len(y_train.unique()),  # Number of unique classes\n",
    "    tree_method=\"hist\",  # Faster training\n",
    "    enable_categorical=True,  # Use categorical support\n",
    "    eval_metric=\"mlogloss\",  # Log loss for multi-class\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=10, verbose=True)\n",
    "\n",
    "print(\"Model training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "# Plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "xgb.plot_importance(xgb_model, importance_type=\"weight\", ax=ax)\n",
    "plt.title(\"Feature Importance (XGBoost)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance scores\n",
    "feature_importance = xgb_model.get_booster().get_score(importance_type=\"weight\")\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "importance_df = pd.DataFrame(\n",
    "    feature_importance.items(),\n",
    "    columns=[\"Feature\", \"Importance\"]\n",
    ").sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Display importance rankings\n",
    "import ace_tools as tools  # Enables interactive display\n",
    "tools.display_dataframe_to_user(name=\"Feature Importance\", dataframe=importance_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math170",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
