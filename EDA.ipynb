{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d1a9796-a876-4906-935a-62dba98854d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nesessary packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "430a47db-916c-4f44-94c3-cea662ec4f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f539a55",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Loan_status.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load data from pickle\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoan_status.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/pickle.py:179\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    180\u001b[0m     filepath_or_buffer,\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    182\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    183\u001b[0m     is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    184\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    185\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    869\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Loan_status.pkl'"
     ]
    }
   ],
   "source": [
    "# Load data from pickle\n",
    "df = pd.read_pickle(\"Loan_status.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d71d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdee608-264f-489d-a49a-55f565d9a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file \n",
    "# df = pd.read_csv(\"Loan_status_2007-2020Q3.gzip\", on_bad_lines=\"skip\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbddff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file to pickle to save time\n",
    "# df.to_pickle(\"Loan_status.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5296ddf6",
   "metadata": {},
   "source": [
    "# Split Dataset into Training, Validation, Testing and Untouched Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1327be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Split off untouched data (10%)\n",
    "df_train_val, df_untouched = train_test_split(df, test_size=0.10, random_state=42)\n",
    "df_train_val = df_train_val.copy()  # Avoid SettingWithCopyWarning\n",
    "df_untouched = df_untouched.copy()\n",
    "df_untouched[\"set_flag\"] = 0  # Mark untouched\n",
    "\n",
    "# Step 2: Split remaining into train (50%), validation (20%), and test (20%)\n",
    "df_train, df_temp = train_test_split(df_train_val, test_size=0.40, random_state=42)\n",
    "df_train = df_train.copy()\n",
    "df_temp = df_temp.copy()\n",
    "df_train[\"set_flag\"] = 1  # Mark train\n",
    "\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.50, random_state=42)\n",
    "df_val = df_val.copy()\n",
    "df_test = df_test.copy()\n",
    "df_val[\"set_flag\"] = 2  # Mark validation\n",
    "df_test[\"set_flag\"] = 3  # Mark test\n",
    "\n",
    "# Combine all subsets back into one dataframe\n",
    "df_final = pd.concat([df_train, df_val, df_test, df_untouched], ignore_index=True)\n",
    "\n",
    "# Save as Parquet for efficient storage\n",
    "df_final.to_parquet(\"dataset_with_flags.parquet\", index=False)\n",
    "\n",
    "# Check dataset sizes\n",
    "print(f\"Training Set: {len(df_train)} rows ({len(df_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation Set: {len(df_val)} rows ({len(df_val)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test Set: {len(df_test)} rows ({len(df_test)/len(df)*100:.1f}%)\")\n",
    "print(f\"Untouched Set: {len(df_untouched)} rows ({len(df_untouched)/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"Data saved with set_flag column.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de7f41-6d5e-41fc-9c03-6a519b8fa707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of training set\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ff141-aac6-4519-bd55-83a558bb0978",
   "metadata": {},
   "source": [
    "# Categorize Predictor Variable to Loss, Good and Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b62edb",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "## 1. Drop features with > 50% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52da6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop features with > 50% missing values\n",
    "missing_percent = (df_train.isna().sum() / len(df_train))* 100\n",
    "cols_to_drop = missing_percent[missing_percent > 50].index\n",
    "print(cols_to_drop)\n",
    "df_train_dropped = df_train.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with string (object) content\n",
    "string_columns = df_train_dropped.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Display the list of column names containing string content\n",
    "string_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8de152",
   "metadata": {},
   "source": [
    "# Convert String to Numeric\n",
    "\n",
    "## 1. Convert id to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b0290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert id to numeric\n",
    "df_train_dropped['id'] = pd.to_numeric(df_train_dropped['id'], errors='coerce').astype('Int64')\n",
    "# check if there'are any NAs\n",
    "df_train_dropped['id'].isna().any()\n",
    "df_train_dropped['id'].isna().sum()\n",
    "\n",
    "# show the column with NA id\n",
    "df_train_dropped[df_train_dropped['id'].isna()]\n",
    "\n",
    "# drop the entire row 39786 because it's empty\n",
    "df_train_dropped = df_train_dropped.drop(39786)\n",
    "df_train_dropped.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeafbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result\n",
    "df_train_dropped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e95601",
   "metadata": {},
   "source": [
    "## 2. Convert emp_title to Numeric (Drop)\n",
    "\n",
    "Reasons to drop feature emp_title:\n",
    "\n",
    "1. High cardinality -- 372,749 unique values in 1,579,764 rows (~24% unique), making it difficult to extract meaningful patterns.\n",
    "2. Encoding challenge -- One-hot encoding is impractical due to excessive feature expansion; label encoding introduces arbitrary ordinal relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9ee85-9a2e-4227-98c0-d57b0d7b50f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop emp_title column (too many distinct emp_titles)\n",
    "df_train_dropped.drop(columns=[\"emp_title\"], inplace=True)\n",
    "\n",
    "df_train_dropped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09e8016",
   "metadata": {},
   "source": [
    "## 3. Convert home_ownership to Numerical (One-hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339eb73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert home_ownership into numerical\n",
    "df_train_dropped['home_ownership'].unique()\n",
    "\n",
    "df_train_dropped['home_ownership'].isnull().sum()\n",
    "\n",
    "# perform one-hot encoding \n",
    "df_train_dropped = pd.get_dummies(df_train_dropped, columns=['home_ownership'], drop_first=False)\n",
    "\n",
    "# convert the true/false into 1/0\n",
    "home_ownership_cols = [col for col in df_train_dropped.columns if col.startswith('home_ownership_')]\n",
    "df_train_dropped[home_ownership_cols] = df_train_dropped[home_ownership_cols].astype(int)\n",
    "df_train_dropped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aa9a5a",
   "metadata": {},
   "source": [
    "## 4. Convert verification_status to Numerical (One-hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d7d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert verification_status\n",
    "df_train_dropped['verification_status'].unique()\n",
    "df_train_dropped['verification_status'].isnull().sum()\n",
    "\n",
    "# one-hot encoding \n",
    "df_train_dropped = pd.get_dummies(df_train_dropped, columns=['verification_status'], drop_first=False)\n",
    "\n",
    "# convert true/false to 1/0\n",
    "verification_status_cols = [col for col in df_train_dropped.columns if col.startswith('verification_status_')]\n",
    "df_train_dropped[verification_status_cols] = df_train_dropped[verification_status_cols].astype(int)\n",
    "df_train_dropped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01928223",
   "metadata": {},
   "source": [
    "## 5. Convert issue_d to Numerical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f9e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert issue_d\n",
    "df_train_dropped['issue_d'].head()\n",
    "\n",
    "df_train_dropped['issue_d'].isnull().sum()\n",
    "\n",
    "df_train_dropped['loan_status'].unique()\n",
    "\n",
    "# Convert issue_d to datetime\n",
    "df_train_dropped['issue_d'] = pd.to_datetime(df_train_dropped['issue_d'], format='%b-%Y')\n",
    "# Group by month-year and calculate the proportion of \"loss\" loans\n",
    "loss_rate = df_train_dropped.groupby(df_train_dropped['issue_d'].dt.to_period('Y'))['loan_category'].apply(lambda x: (x == 'Loss').mean())\n",
    "\n",
    "# Plot the trend\n",
    "plt.figure(figsize=(14, 6))\n",
    "#x_labels = loss_rate.index.astype(str)[::4]  # Show every 4th label\n",
    "plt.plot(loss_rate.index.astype(str), loss_rate.values, marker='o', linestyle='-')\n",
    "#plt.xticks(ticks=x_labels, rotation=45)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Loan Issue Date (Year)\")\n",
    "plt.ylabel(\"Proportion of Loss Loans\")\n",
    "plt.title(\"Trend of Loan Default Rate Over Time (Yearly)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b214273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert issue_d to an Ordinal Feature\n",
    "df_train_dropped['issue_d_ordinal'] = df_train_dropped['issue_d'].dt.year - df_train_dropped['issue_d'].dt.year.min()\n",
    "#df_train_dropped.drop(columns=['issue_d'], inplace=True)  # Drop original datetime column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19d525",
   "metadata": {},
   "source": [
    "## 6. Convert url to Numeric (Drop)\n",
    "Reason to drop url: \n",
    "1. url unaccessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loanDetails that requires investor account to login (Unaccessible)\n",
    "## Drop column\n",
    "df_train_dropped.drop(columns=[\"url\"], inplace=True)\n",
    "\n",
    "df_train_dropped.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10629859",
   "metadata": {},
   "source": [
    "## 7. Convert Term to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55091bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature Term and convert to numeric\n",
    "df_train_dropped[\"term\"].unique()\n",
    "\n",
    "# Convert  numeric \n",
    "df_train_dropped[\"term\"] = df_train_dropped[\"term\"].str.extract(\"(\\d+)\").astype(float)\n",
    "\n",
    "# Plot distribution of loan terms\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=df_train_dropped[\"term\"])\n",
    "plt.title(\"Distribution of Loan Terms\")\n",
    "plt.xlabel(\"Loan Term (Months)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ce3da",
   "metadata": {},
   "source": [
    "## 8. Convert int_rate to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature int_rate and convert to numeric\n",
    "df_train_dropped[\"int_rate\"].unique()\n",
    "\n",
    "# Plot distribution of int_rate\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=df_train_dropped[\"int_rate\"])\n",
    "plt.title(\"Distribution of int_rate\")\n",
    "plt.xlabel(\"int_rate)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# Convert\n",
    "df_train_dropped[\"int_rate\"] = df_train_dropped[\"int_rate\"].replace(\"nan\", np.nan)\n",
    "df_train_dropped[\"int_rate\"] = df_train_dropped[\"int_rate\"].str.replace(\"%\", \"\").astype(float) / 100\n",
    "df_train_dropped[\"int_rate\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f075dc2b",
   "metadata": {},
   "source": [
    "## 9. Convert sub_grade to Numeric (Drop grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607543c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature grade and subgrade\n",
    "df_train_dropped[\"grade\"].unique()\n",
    "df_train_dropped[\"sub_grade\"].unique()\n",
    "\n",
    "# Possible feature engineering: Combine into one feature A=1, B=2, C=3, D=4, E=5, F=6, G=7 (Smaller number has lower risk)===> \n",
    "# Use only converted sub_grade, drop feature grade\n",
    "df_train_dropped = df_train_dropped.drop([\"grade\"], axis=1)\n",
    "\n",
    "# Define base values for grades (lower = better credit, higher = higher risk)\n",
    "grade_mapping = {\"A\" :1, \"B\" : 2, \"C\" : 3, \"D\" : 4, \"E\" : 5, \"F\" : 6, \"G\" : 7}\n",
    "\n",
    "# Convert nan to np.nan\n",
    "df_train_dropped[\"sub_grade\"] = df_train_dropped[\"sub_grade\"].replace(\"nan\", np.nan)\n",
    "\n",
    "# Check if has nan valus (1 nan)\n",
    "df_train_dropped[\"sub_grade\"].isna().sum()\n",
    "\n",
    "# Check if the original \"nan\" value converted to np.nan (All converted)\n",
    "print((df_train_dropped[\"sub_grade\"] == \"nan\").sum())\n",
    "\n",
    "# Plot the distribution of sub_grade with proper ranking from low to high\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(y=df_train_dropped[\"sub_grade\"], order=sorted(df_train_dropped[\"sub_grade\"].unique()), palette=\"Blues_r\")\n",
    "plt.title(\"Distribution of Sub Grade (Ranked Low to High)\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Sub Grade\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Convert sub_grade into an ordered numeric feature where A1 is lowest risk and G5 is highest risk\n",
    "df_train_dropped[\"sub_grade\"] = df_train_dropped[\"sub_grade\"].apply(lambda x: grade_mapping[str(x)[0]] * 10 + int(str(x)[1]) if pd.notna(x) else np.nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4186b7",
   "metadata": {},
   "source": [
    "## 10. Convert emp_length to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature emp_length\n",
    "df_train_dropped[\"emp_length\"].unique()\n",
    "\n",
    "def convert_emp_length(emp):\n",
    "    if pd.isna(emp):  # Handle missing values\n",
    "        return np.nan\n",
    "    if emp == \"10+ years\":\n",
    "        return 10\n",
    "    elif emp == \"< 1 year\":\n",
    "        return 0\n",
    "    else:\n",
    "        return int(emp.split()[0])  # Extract the number from \"X years\"\n",
    "## Check below for convertion criterion\n",
    "\n",
    "\n",
    "## Convert emp_length to numeric\n",
    "df_train_dropped[\"emp_length\"] = df_train_dropped[\"emp_length\"].apply(convert_emp_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handeling 'purpose', 'title', 'zip_code', 'addr_state',\n",
    "\n",
    "df_train_dropped[\"purpose\"].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9279e",
   "metadata": {},
   "source": [
    "## 11. Convert Purpose to Numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d11977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"purpose\" from string to numeric\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'purpose' column\n",
    "df_train_dropped[\"purpose\"] = le.fit_transform(df_train_dropped[\"purpose\"])\n",
    "\n",
    "# View unique mappings\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26761c",
   "metadata": {},
   "source": [
    "## 12.Convert Title to Numeric (Drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2746072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'title' from strings to numeric (Since similar to \"Purpose, drop it\")\n",
    "num_unique_titles = df_train_dropped[\"title\"].nunique()\n",
    "title_counts = df_train_dropped[\"title\"].value_counts()\n",
    "purpose_counts = df_train_dropped[\"purpose\"].value_counts()\n",
    "\n",
    "df_train_dropped = df_train_dropped.drop(columns=[\"title\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a88ee",
   "metadata": {},
   "source": [
    "## 13. Convert zip_code, addr_state to Numeric(Drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5537422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"zip_code\" to numeric\n",
    "df_train_dropped[\"zip_code\"].unique()\n",
    "\n",
    "# Drop zip_code\n",
    "df_train_dropped = df_train_dropped.drop(columns=[\"zip_code\"])\n",
    "\n",
    "# Drop \"addr_state\"\n",
    "df_train_dropped[\"addr_state\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243f606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check df after column dropping\n",
    "df_train_dropped.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
